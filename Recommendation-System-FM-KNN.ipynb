{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook outlines how to build a recommendation system using SageMaker's Factorization Machines (FM). The main goal is to showcase how to extend FM model to predict top \"X\" recommendations using SageMaker's KNN and Batch Transform.\n",
    "\n",
    "There are four parts to this notebook:\n",
    "\n",
    "1. Building a FM Model\n",
    "2. Repackaging FM Model to fit a KNN Model\n",
    "3. Building a KNN model\n",
    "4. Running Batch Transform for predicting top \"X\" items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Building a FM Model using movie lens dataset\n",
    "\n",
    "Julien Simon has written a fantastic blog about how to build a FM model using SageMaker with detailed explanation. Please see the links below for more information. In this part, I utilized his code for the most part to have continutity for performing additional steps.\n",
    "\n",
    "Source - https://aws.amazon.com/blogs/machine-learning/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import json_deserializer\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "import pandas as pd\n",
    "import boto3, io, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download movie rating data from movie lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-27 19:51:25--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4924029 (4.7M) [application/zip]\n",
      "Saving to: ‘ml-100k.zip’\n",
      "\n",
      "ml-100k.zip         100%[===================>]   4.70M  8.25MB/s    in 0.6s    \n",
      "\n",
      "2019-08-27 19:51:26 (8.25 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
      "\n",
      "Archive:  ml-100k.zip\n",
      "   creating: ml-100k/\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    }
   ],
   "source": [
    "#download data\n",
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/recommedation_forecast/ml-100k\n"
     ]
    }
   ],
   "source": [
    "%cd ml-100k\n",
    "!shuf ua.base -o ua.base.shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>885</td>\n",
       "      <td>423</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>244</td>\n",
       "      <td>208</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>655</td>\n",
       "      <td>650</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>79</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>678</td>\n",
       "      <td>742</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating\n",
       "0      885       423       4\n",
       "1      244       208       5\n",
       "2      655       650       3\n",
       "3       60        79       4\n",
       "4      678       742       4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_movie_ratings_train = pd.read_csv('ua.base.shuffled', sep='\\t', index_col=False, \n",
    "                 names=['user_id' , 'movie_id' , 'rating'])\n",
    "user_movie_ratings_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>117</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating\n",
       "0        1        20       4\n",
       "1        1        33       4\n",
       "2        1        61       4\n",
       "3        1       117       3\n",
       "4        1       155       2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_movie_ratings_test = pd.read_csv('ua.test', sep='\\t', index_col=False, \n",
    "                 names=['user_id' , 'movie_id' , 'rating'])\n",
    "user_movie_ratings_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of users:  943\n",
      " # of movies:  1682\n",
      " Training Count:  90570\n",
      " Test Count:  9430\n",
      " Features (# of users + # of movies):  2625\n"
     ]
    }
   ],
   "source": [
    "nb_users= user_movie_ratings_train['user_id'].max()\n",
    "nb_movies=user_movie_ratings_train['movie_id'].max()\n",
    "nb_features=nb_users+nb_movies\n",
    "nb_ratings_test=len(user_movie_ratings_test.index)\n",
    "nb_ratings_train=len(user_movie_ratings_train.index)\n",
    "print \" # of users: \", nb_users\n",
    "print \" # of movies: \", nb_movies\n",
    "print \" Training Count: \", nb_ratings_train\n",
    "print \" Test Count: \", nb_ratings_test\n",
    "print \" Features (# of users + # of movies): \", nb_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM Input\n",
    "\n",
    "Input to FM is a one-hot encoded sparse matrix. Only ratings 4 and above are considered for the model. We will be ignoring ratings 3 and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(df, lines, columns):\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((lines, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line=0\n",
    "    for index, row in df.iterrows():\n",
    "            X[line,row['user_id']-1] = 1\n",
    "            X[line, nb_users+(row['movie_id']-1)] = 1\n",
    "            if int(row['rating']) >= 4:\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "            line=line+1\n",
    "\n",
    "    Y=np.array(Y).astype('float32')            \n",
    "    return X,Y\n",
    "\n",
    "\n",
    "X_train, Y_train = loadDataset(user_movie_ratings_train, nb_ratings_train, nb_features)\n",
    "X_test, Y_test = loadDataset(user_movie_ratings_test, nb_ratings_test, nb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 2625)\n",
      "(90570,)\n",
      "Training labels: 49906 zeros, 40664 ones\n",
      "(9430, 2625)\n",
      "(9430,)\n",
      "Test labels: 5469 zeros, 3961 ones\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "assert X_train.shape == (nb_ratings_train, nb_features)\n",
    "assert Y_train.shape == (nb_ratings_train, )\n",
    "zero_labels = np.count_nonzero(Y_train)\n",
    "print(\"Training labels: %d zeros, %d ones\" % (zero_labels, nb_ratings_train-zero_labels))\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "assert X_test.shape  == (nb_ratings_test, nb_features)\n",
    "assert Y_test.shape  == (nb_ratings_test, )\n",
    "zero_labels = np.count_nonzero(Y_test)\n",
    "print(\"Test labels: %d zeros, %d ones\" % (zero_labels, nb_ratings_test-zero_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Protobuf format for saving to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this value to your own bucket name\n",
    "bucket = 'recommendationforecast'\n",
    "prefix = 'fm'\n",
    "\n",
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data S3 path:  s3://recommendationforecast/fm/train/train.protobuf\n",
      "Test data S3 path:  s3://recommendationforecast/fm/test/test.protobuf\n",
      "FM model output S3 path: s3://recommendationforecast/fm/output\n"
     ]
    }
   ],
   "source": [
    "def writeDatasetToProtobuf(X, bucket, prefix, key, d_type, Y=None):\n",
    "    buf = io.BytesIO()\n",
    "    if d_type == \"sparse\":\n",
    "        smac.write_spmatrix_to_sparse_tensor(buf, X, labels=Y)\n",
    "    else:\n",
    "        smac.write_numpy_to_dense_tensor(buf, X, labels=Y)\n",
    "        \n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "    \n",
    "fm_train_data_path = writeDatasetToProtobuf(X_train, bucket, train_prefix, train_key, \"sparse\", Y_train)    \n",
    "fm_test_data_path  = writeDatasetToProtobuf(X_test, bucket, test_prefix, test_key, \"sparse\", Y_test)    \n",
    "  \n",
    "print \"Training data S3 path: \",fm_train_data_path\n",
    "print \"Test data S3 path: \",fm_test_data_path\n",
    "print \"FM model output S3 path: {}\".format(output_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training job\n",
    "\n",
    "You can play around with the hyper parameters until you are happy with the prediction. For this dataset and hyper parameters configuration, after 100 epochs, test accuracy was around 70% on average and the F1 score (a typical metric for a binary classifier) was around 0.74 (1 indicates a perfect classifier). Not great, but you can fine tune the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-27 20:39:59 Starting - Starting the training job...\n",
      "2019-08-27 20:40:01 Starting - Launching requested ML instances......\n",
      "2019-08-27 20:41:07 Starting - Preparing the instances for training...."
     ]
    }
   ],
   "source": [
    "instance_type='ml.m5.large'\n",
    "fm = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"factorization-machines\"),\n",
    "                                   get_execution_role(), \n",
    "                                   train_instance_count=1, \n",
    "                                   train_instance_type=instance_type,\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=nb_features,\n",
    "                      predictor_type='binary_classifier',\n",
    "                      mini_batch_size=1000,\n",
    "                      num_factors=128,\n",
    "                      epochs=200)\n",
    "\n",
    "fm.fit({'train': fm_train_data_path, 'test': fm_test_data_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "recommendation_classifier = fm.deploy(initial_instance_count = 1,  instance_type = instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://recommendationforecast/fm/test/test.protobuf\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print fm_test_data_path\n",
    "#results = recommendation_classifier.predict(json.dumps(fm_test_data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Repackaging Model data to fit a KNN Model\n",
    "\n",
    "Now that we have the model created and stored in SageMaker, we can download the same and repackage it to fit a KNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Path:  s3://recommendation-system-12-06/fm/output/factorization-machines-2019-01-06-19-27-02-023/output/model.tar.gz\n",
      "/home/ec2-user/SageMaker/AmazonSageMaker-rama-ml/fm-based-recommendation-system\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "model_file_name = \"model.tar.gz\"\n",
    "model_full_path = fm.output_path +\"/\"+ fm.latest_training_job.job_name +\"/output/\"+model_file_name\n",
    "print \"Model Path: \", model_full_path\n",
    "\n",
    "#Download FM model \n",
    "%cd ..\n",
    "os.system('aws s3 cp '+model_full_path+ './')\n",
    "\n",
    "#Extract model file for loading to MXNet\n",
    "os.system('tar xzvf '+model_file_name)\n",
    "os.system(\"unzip -o model_algo-1\")\n",
    "os.system(\"mv symbol.json model-symbol.json\")\n",
    "os.system(\"mv params model-0000.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract model data to create item and user latent matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract model data\n",
    "m = mx.module.Module.load('./model', 0, False, label_names=['out_label'])\n",
    "V = m._arg_params['v'].asnumpy()\n",
    "w = m._arg_params['w1_weight'].asnumpy()\n",
    "b = m._arg_params['w0_weight'].asnumpy()\n",
    "\n",
    "# item latent matrix - concat(V[i], w[i]).  \n",
    "knn_item_matrix = np.concatenate((V[nb_users:], w[nb_users:]), axis=1)\n",
    "knn_train_label = np.arange(1,nb_movies+1)\n",
    "\n",
    "#user latent matrix - concat (V[u], 1) \n",
    "ones = np.ones(nb_users).reshape((nb_users, 1))\n",
    "knn_user_matrix = np.concatenate((V[:nb_users], ones), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Building KNN Model\n",
    "\n",
    "In this section, we upload the model input data to S3, create a KNN model and save the same. Saving the model, will display the model in the model section of SageMaker. Also, it will aid in calling batch transform down the line or even deploying it as an end point for real-time inference.\n",
    "\n",
    "This approach uses the default 'index_type' parameter for knn. It is precise but can be slow for large datasets. In such cases, you may want to use a different 'index_type' parameter leading to an approximate, yet fast answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KNN train features shape = ', (1682, 65))\n",
      "uploaded KNN train data: s3://recommendation-system-12-06/knn/train.protobuf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: knn-2019-01-06-19-40-56-509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-06 19:40:56 Starting - Starting the training job...\n",
      "2019-01-06 19:41:01 Starting - Launching requested ML instances......\n",
      "2019-01-06 19:42:04 Starting - Preparing the instances for training......\n",
      "2019-01-06 19:43:23 Downloading - Downloading input data...\n",
      "2019-01-06 19:43:57 Training - Training image download completed. Training in progress.\n",
      "2019-01-06 19:43:57 Uploading - Uploading generated training model.\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'index_metric': u'L2', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'_log_level': u'info', u'faiss_index_ivf_nlists': u'auto', u'epochs': u'1', u'index_type': u'faiss.Flat', u'_faiss_index_nprobe': u'5', u'_kvstore': u'dist_async', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000'}\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'sample_size': u'200000', u'feature_dim': u'65', u'index_metric': u'INNER_PRODUCT', u'k': u'100', u'predictor_type': u'classifier'}\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] Final configuration: {u'index_metric': u'INNER_PRODUCT', u'predictor_type': u'classifier', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'_log_level': u'info', u'feature_dim': u'65', u'faiss_index_ivf_nlists': u'auto', u'sample_size': u'200000', u'epochs': u'1', u'index_type': u'faiss.Flat', u'_faiss_index_nprobe': u'5', u'_kvstore': u'dist_async', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'k': u'100'}\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 WARNING 139855324034880] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/62ea632a-2aa9-47fd-b5dd-689e08e139c0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'knn-2019-01-06-19-40-56-509', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '1', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/fda4499c-7378-4156-af3d-40612c7bd90d', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/62ea632a-2aa9-47fd-b5dd-689e08e139c0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '1', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'knn-2019-01-06-19-40-56-509', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '1', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/fda4499c-7378-4156-af3d-40612c7bd90d', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] Launching parameter server for role server\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/62ea632a-2aa9-47fd-b5dd-689e08e139c0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'knn-2019-01-06-19-40-56-509', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '1', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/fda4499c-7378-4156-af3d-40612c7bd90d', 'PWD': '/', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/62ea632a-2aa9-47fd-b5dd-689e08e139c0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '1', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'knn-2019-01-06-19-40-56-509', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '1', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/fda4499c-7378-4156-af3d-40612c7bd90d', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/62ea632a-2aa9-47fd-b5dd-689e08e139c0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '1', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-west-2', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'knn-2019-01-06-19-40-56-509', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '1', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/fda4499c-7378-4156-af3d-40612c7bd90d', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[31mProcess 32 is a shell:scheduler.\u001b[0m\n",
      "\u001b[31mProcess 33 is a shell:server.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] Using default worker.\u001b[0m\n",
      "\u001b[31m[2019-01-06 19:43:54.900] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/train\", \"num_examples\": 5000, \"features\": [{\"name\": \"label_values\", \"shape\": [1], \"storage_type\": \"dense\"}, {\"name\": \"values\", \"shape\": [65], \"storage_type\": \"dense\"}]}\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] nvidia-smi took: 0.0508320331573 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:54 INFO 139855324034880] Create Store: dist_async\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:55 ERROR 139855324034880] nvidia-smi: failed to run (127): /bin/sh: nvidia-smi: command not found\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:55 INFO 139855324034880] Using per-worker sample size = 200000 (Available virtual memory = 6491938816 bytes, GPU free memory = 0 bytes, number of workers = 1). If an out-of-memory error occurs, choose a larger instance type, use dimension reduction, decrease sample_size, and/or decrease mini_batch_size.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1546803835.9145, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\"}, \"StartTime\": 1546803835.914455}\n",
      "\u001b[0m\n",
      "\u001b[31m[2019-01-06 19:43:55.914] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 1014, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:55 INFO 139855324034880] push reservoir to kv... 1 num_workers 0 rank\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:55 INFO 139855324034880] ...done (1682)\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:55 INFO 139855324034880] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1682, \"sum\": 1682.0, \"min\": 1682}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1682, \"sum\": 1682.0, \"min\": 1682}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1682, \"sum\": 1682.0, \"min\": 1682}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1546803835.965863, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\", \"epoch\": 0}, \"StartTime\": 1546803835.914822}\n",
      "\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:55 INFO 139855324034880] #throughput_metric: host=algo-1, train throughput=32859.2689638 records/second\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:56 INFO 139855324034880] pulled row count... worker 0 rows 1682\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:56 INFO 139855324034880] pulled... worker 0 data (1682, 65) labels (1682, 1) nans 0\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:56 INFO 139855324034880] calling index.train...\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:56 INFO 139855324034880] ...done calling index.train\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:56 INFO 139855324034880] calling index.add...\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:56 INFO 139855324034880] ...done calling index.add\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"model.serialize.time\": {\"count\": 1, \"max\": 1.2941360473632812, \"sum\": 1.2941360473632812, \"min\": 1.2941360473632812}, \"finalize.time\": {\"count\": 1, \"max\": 202.5599479675293, \"sum\": 202.5599479675293, \"min\": 202.5599479675293}, \"initialize.time\": {\"count\": 1, \"max\": 1006.9239139556885, \"sum\": 1006.9239139556885, \"min\": 1006.9239139556885}, \"update.time\": {\"count\": 1, \"max\": 50.3997802734375, \"sum\": 50.3997802734375, \"min\": 50.3997802734375}}, \"EndTime\": 1546803836.170104, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\"}, \"StartTime\": 1546803834.89894}\n",
      "\u001b[0m\n",
      "\u001b[31m[01/06/2019 19:43:56 INFO 139855324034880] Test data is not provided.\u001b[0m\n",
      "\u001b[31m[2019-01-06 19:43:56.170] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 255, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[31m[2019-01-06 19:43:56.170] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"duration\": 1269, \"num_epochs\": 2, \"num_examples\": 2}\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 1659.825086593628, \"sum\": 1659.825086593628, \"min\": 1659.825086593628}, \"setuptime\": {\"count\": 1, \"max\": 21.05093002319336, \"sum\": 21.05093002319336, \"min\": 21.05093002319336}}, \"EndTime\": 1546803836.17095, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\"}, \"StartTime\": 1546803836.170173}\n",
      "\u001b[0m\n",
      "\n",
      "2019-01-06 19:44:05 Completed - Training job completed\n",
      "Billable seconds: 43\n",
      "created model:  knn-2019-01-06-19-40-56-509\n",
      "saved the model\n"
     ]
    }
   ],
   "source": [
    "print('KNN train features shape = ', knn_item_matrix.shape)\n",
    "knn_prefix = 'knn'\n",
    "knn_output_prefix  = 's3://{}/{}/output'.format(bucket, knn_prefix)\n",
    "knn_train_data_path = writeDatasetToProtobuf(knn_item_matrix, bucket, knn_prefix, train_key, \"dense\", knn_train_label)\n",
    "print('uploaded KNN train data: {}'.format(knn_train_data_path))\n",
    "\n",
    "nb_recommendations = 100\n",
    "\n",
    "# set up the estimator\n",
    "knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "    get_execution_role(),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=instance_type,\n",
    "    output_path=knn_output_prefix,\n",
    "    sagemaker_session=sagemaker.Session())\n",
    "\n",
    "knn.set_hyperparameters(feature_dim=knn_item_matrix.shape[1], k=nb_recommendations, index_metric=\"INNER_PRODUCT\", predictor_type='classifier', sample_size=200000)\n",
    "fit_input = {'train': knn_train_data_path}\n",
    "knn.fit(fit_input)\n",
    "knn_model_name =  knn.latest_training_job.job_name\n",
    "print \"created model: \", knn_model_name\n",
    "\n",
    "# save the model so that we can reference it in the next step during batch inference\n",
    "sm = boto3.client(service_name='sagemaker')\n",
    "primary_container = {\n",
    "    'Image': knn.image_name,\n",
    "    'ModelDataUrl': knn.model_data,\n",
    "}\n",
    "\n",
    "knn_model = sm.create_model(\n",
    "        ModelName = knn.latest_training_job.job_name,\n",
    "        ExecutionRoleArn = knn.role,\n",
    "        PrimaryContainer = primary_container)\n",
    "print \"saved the model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Batch Transform\n",
    "\n",
    "In this section, we will use SageMaker's batch transform option to batch predict top X for all the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: knn-2019-01-06-19-48-05-338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inference data path:  s3://recommendation-system-12-06/knn/train.protobuf\n",
      "..........................................!\n"
     ]
    }
   ],
   "source": [
    "#upload inference data to S3\n",
    "knn_batch_data_path = writeDatasetToProtobuf(knn_user_matrix, bucket, knn_prefix, train_key, \"dense\")\n",
    "print \"Batch inference data path: \",knn_batch_data_path\n",
    "\n",
    "# Initialize the transformer object\n",
    "transformer =sagemaker.transformer.Transformer(\n",
    "    base_transform_job_name=\"knn\",\n",
    "    model_name=knn_model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=knn_output_prefix,\n",
    "    accept=\"application/jsonlines; verbose=true\"\n",
    ")\n",
    "\n",
    "# Start a transform job:\n",
    "transformer.transform(knn_batch_data_path, content_type='application/x-recordio-protobuf')\n",
    "transformer.wait()\n",
    "\n",
    "\n",
    "#Download predictions \n",
    "results_file_name = \"inference_output\"\n",
    "inference_output_file = \"knn/output/train.protobuf.out\"\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.download_file(bucket, inference_output_file, results_file_name)\n",
    "with open(results_file_name) as f:\n",
    "    results = f.readlines()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movie Ids for user #90 : [268, 89, 23, 923, 655, 165, 423, 192, 509, 28, 87, 193, 69, 493, 176, 208, 269, 705, 482, 527, 180, 183, 966, 166, 173, 216, 211, 520, 489, 124, 96, 9, 83, 246, 168, 1, 659, 185, 194, 56, 251, 519, 196, 132, 190, 316, 197, 204, 510, 302, 210, 484, 435, 134, 496, 285, 641, 181, 478, 170, 654, 963, 136, 523, 187, 223, 275, 1142, 313, 100, 1039, 651, 22, 408, 480, 498, 515, 178, 603, 79, 272, 191, 199, 114, 169, 127, 172, 427, 657, 357, 511, 318, 12, 174, 513, 98, 479, 50, 483, 64]\n",
      "\n",
      "Movie distances for user #90 : [2.5401, 2.5459, 2.5539, 2.5628, 2.5637, 2.5671, 2.5817, 2.583, 2.5832, 2.5901, 2.5923, 2.5979, 2.6317, 2.6386, 2.6395, 2.6731, 2.677, 2.6853, 2.6866, 2.6995, 2.6997, 2.7006, 2.7317, 2.7437, 2.7449, 2.7922, 2.8071, 2.8205, 2.8214, 2.8386, 2.839, 2.8512, 2.8675, 2.8778, 2.8779, 2.8942, 2.9031, 2.9307, 2.9561, 2.9647, 2.9713, 2.9755, 2.9921, 2.9982, 2.9991, 3.0015, 3.0291, 3.0665, 3.0714, 3.0882, 3.0998, 3.1148, 3.1159, 3.1234, 3.1323, 3.1589, 3.1631, 3.2076, 3.2336, 3.2404, 3.2407, 3.2466, 3.2897, 3.3141, 3.3157, 3.3169, 3.3232, 3.3453, 3.4406, 3.4503, 3.4703, 3.4873, 3.4876, 3.5185, 3.5436, 3.5827, 3.6038, 3.6049, 3.6562, 3.6751, 3.7038, 3.7189, 3.7386, 3.769, 3.8471, 4.2482, 4.2502, 4.263, 4.2964, 4.3168, 4.3831, 4.3927, 4.416, 4.4293, 4.4504, 4.6206, 4.7173, 4.8195, 4.8528, 5.2139]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "test_user_idx = 89\n",
    "u_one_json = json.loads(results[test_user_idx])\n",
    "\n",
    "print \"Recommended movie Ids for user #{} : {}\".format(test_user_idx+1, [int(movie_id) for movie_id in u_one_json['labels']])\n",
    "print\n",
    "print \"Movie distances for user #{} : {}\".format(test_user_idx+1,  [round(distance, 4) for distance in u_one_json['distances']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
